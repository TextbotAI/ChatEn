# ChatEn

Making experiments with gpt-2 we see that this network is good for believable text synthesis because unlike BERT it was successfully trained by the sum of predicting the next token. Unfortunately the process of text generation is uncontrolled. ChatEn must fix the situation.

We use gpt-2 774M (774 million of trainable parameters) network with more than two hundred layers and about 50000 bpe tokens.

License

MIT
