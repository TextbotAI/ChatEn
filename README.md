# ChatEn

Making experiments with gpt-2 we see that this network is good for believable text synthesis because unlike BERT it was successfully trained by the sum of predicting the next token. Unfortunately the process of text generation is uncontrolled. ChatEn must fix the situation.
